<p align="center">
    <img src="./assets/logo.jpg" alt="logo">
</p>

# Easy-LLM-Evaluator: One-Click AI-enhanced Evaluation for LLM Finetuning

![License](https://img.shields.io/github/license/Antlera/easy-llm-evaluator)

This project streamlines the process of evaluating and comparing the performance of base and fine-tuned machine learning models. It automatically generates test questions using GPT, obtains responses from the models, assesses these responses using a GPT model, and produces a score indicating the model's effectiveness. The evaluation results are then transformed into a format suitable for web-based visualization. This project enables rapid, consistent, and automated evaluation of language models, providing data-driven insights for model training and fine-tuning."

## Usage

First, install the required dependencies:

```bash
pip install -r requirements.txt
```

Then, update the run_eval.sh script with your specific model and project details:

```bash
export BASE_MODEL_PATH="/path/to/base_model"
export TUNED_MODEL_PATH="/path/to/tuned_model"
export BASE_MODEL_NAME="base_model_name"
export TUNED_MODEL_NAME="tuned_model_name"
export PROJECT_NAME="project_name"
export CATEGORY="category"
```

Replace the placeholder values with your own corresponding data:

```bash
BASE_MODEL_PATH: Set the path to the base model.  For example, `export BASE_MODEL_PATH="models/vicuna-7b/"` .
TUNED_MODEL_PATH: Set the path to the tuned model.  For example, `export TUNED_MODEL_PATH="models/vicuna-7b-CrimeKGAssitantClean/"` .
BASE_MODEL_NAME: Set the name of the base model.  For example, `export PROJECT_NAME="vicuna-7b"` .
TUNED_MODEL_NAME: Set the name of the tuned model.  For example, `export PROJECT_NAME="vicuna-7b-CrimeKGAssitantClean"` .
PROJECT_NAME: Set the name of the project.  For example, `export PROJECT_NAME="vicuna-7b-law"` .
CATEGORY: Set the category. For example, `export CATEGORY="Legal Q&A"` .
````

Finally, run the one-click evaluation script:

```bash
source eval/script/run_eval.sh
```

## Workflow
1. **Extract Subset from Source Test Data**: The script uses a random or stratified sampling method to draw a representative subset from the source test dataset.
2. **Generate Test Questions with GPT**: The chosen subset serves as a seed for the GPT model to generate additional test questions, thereby expanding the test dataset.
3. **Answer Generation by Base and Fine-tuned Models**: Both the base model and the fine-tuned model use the extended test dataset to generate corresponding answers.
4. **Performance Evaluation by GPT**: The answers generated by both models are evaluated by the GPT model. This evaluation produces a score that represents the effectiveness of each model.
5. **Visualize Review Result**: The evaluation and scoring data is processed and transformed into a format suitable for displaying on a frontend website.

## Extract Subset from Source Test Data

A carefully curated subset of the source test data is drawn using random or stratified sampling techniques to represent the diversity of the data. This subset serves as the seed data for the GPT model to generate test questions.

### File structure

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ input
                └─ vicuna-7b-law
                    └─ source_data.json
```

### Sourse Test Data Example

`source_data.json` Contains test data during fine tuning.

```json
[
    {
        "output": "年9月到明年8月底，租期一年。",
        "input": "你好。我昨晚刚租了房子，签了房屋租赁合同，付一押一，合同签的是今年9.?"
    },
    {
        "output": "是的，无期之前的年数算作有期刑的一部分。",
        "input": "你好，判无期之前住了4年，转有期之后，无期之前的年数在有期之内吗?"
    },
    ······Omitted Data······
    {
        "output": "发生交通事故应该去事故发生地的基层人民法院起诉。",
        "input": "发生交通事故应该去哪个法院起诉?"
    },
    {
        "output": "请您提供更具体的情况，无法回答。同时，强烈建议您立即向当地公安机关报案，以维护自己的合法权益。",
        "input": "我遇到一个流氓无赖，起先加了好友，聊天以后有好感，就见面了，?"
    }
    
]
```

## Generate Test Questions with GPT

The GPT model uses the seed data to generate a broad range of test questions, effectively expanding the diversity of the test dataset. The questions generated during this stage assess the models' comprehension, contextual understanding, and response generation capabilities.

### File structure

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ question
                └─ vicuna-7b-law
                    └─ example_questions.json
                    └─ questions.jsonl
```
### Randomly Picked Seed Questions

`example_questions.json` Contains the seed questions used to generate the problem.

```bash
[
    {
    "output": "根据《中华人民共和国道路交通安全法》第二十六条的规定，机动车行驶过程中发生交通事故造成他人伤亡或者财产损失的，应当依法承担赔偿责任。 \n\n根据您提供的情况来看，交通事故造成人员骨折、错位、粉碎性骨头渣等损伤，应当由机动车主依法承担赔偿责任。 \n\n另外，明确责任划分为55，并且保险公司只赔付50%的情况下，剩余的50%责任应当由机动车主承担。",
    "input": "交通事故导致人伤、骨折、错位，里面有粉碎性骨头渣、责任划分为55、保险公司只赔付百分之50那剩下的百分之50是不是有机动车主承担?"
    },
    {
    "output": "由于每个案件和每个犯人的情况不同，无法确定具体的出狱时间。需要等待法院的审判和判决，以及拘留所的处理程序。建议家属或律师可以咨询相关部门或律师，以了解该犯人的案件情况和出狱可能性。",
    "input": "你好，累犯在犯刑事案件，己被批捕，人己在拘留所，请问什么时候出来?"
    }
]
```

### GPT-Generated Qusestions with Seed Question

`questions.jsonl` Contains seed questions and questions generated from the seed questions.

#### Extracted seed problem

```json
{"question_id": 2, "text": "交通事故导致人伤、骨折、错位，里面有粉碎性骨头渣、责任划分为55、保险公司只赔付百分之50那剩下的百分之50是不是有机动车主承担?", "category": "法律问答"}
{"question_id": 3, "text": "你好，累犯在犯刑事案件，己被批捕，人己在拘留所，请问什么时候出来?", "category": "法律问答"}
```

#### Questions generated by gpt based on seed questions
```json
{"question_id": 42, "text": "如果某人在行驶中发生交通事故，但没有及时报警，对其后续的索赔和赔偿有什么影响？", "category": "法律问答_GPT"}
{"question_id": 43, "text": "如果在刑事案件中被判有罪，有哪些因素会影响判决的结果？", "category": "法律问答_GPT"}
```

## Answer Generation by Base and Fine-tuned Models

The base and fine-tuned models process the test questions to generate responses. These responses are then used to compare and evaluate the performance of the models, providing a direct measure of the effectiveness of the fine-tuning process.

### File structure

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ answer
                └─ vicuna-7b-law
                    └─ answer_vicuna-7b-CrimeKGAssitantClean.jsonl
                    └─ answer_vicuna-7b.jsonl
```

## Performance Evaluation by GPT
The answers generated by both the base and the fine-tuned models are fed into a GPT model for evaluation. This process produces a quantitative score that reflects each model's performance and effectiveness.

### File structure

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ review
                └─ vicuna-7b-law
                    └─ review_vicuna-7b-CrimeKGAssitantClean_vicuna-7b.jsonl
```
## Visualize Review Result

The evaluation and scoring data is transformed into a format suitable for visualization on a frontend web interface. This step helps to translate complex performance metrics into easy-to-understand visual data, providing a comprehensive overview of the models' performance.

<p align="center">
    <img src="./assets/eval.jpg" alt="logo">
</p>

<p align="center">
    <img src="./assets/eval_gpt.jpg" alt="logo">
</p>

