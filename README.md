<p align="center">
    <img src="./assets/logo.jpg" alt="logo">
</p>

# Easy-LLM-Evaluator: One-Click AI-enhanced Evaluation for LLM Finetuning

![License](https://img.shields.io/github/license/Antlera/easy-llm-evaluator)

This project streamlines the process of evaluating and comparing the performance of base and fine-tuned machine learning models. It automatically generates test questions using GPT, obtains responses from the models, assesses these responses using a GPT model, and produces a score indicating the model's effectiveness. The evaluation results are then transformed into a format suitable for web-based visualization. This project enables rapid, consistent, and automated evaluation of language models, providing data-driven insights for model training and fine-tuning."

## Usage

First, install the required dependencies:

```bash
pip install -r requirements.txt
```

Then, update the run_eval.sh script with your specific model and project details:

```bash
export BASE_MODEL_PATH="/path/to/base_model"
export TUNED_MODEL_PATH="/path/to/tuned_model"
export BASE_MODEL_NAME="base_model_name"
export TUNED_MODEL_NAME="tuned_model_name"
export PROJECT_NAME="project_name"
export CATEGORY="category"
```

Replace the placeholder values with your own corresponding data:

```bash
BASE_MODEL_PATH: Set the path to the base model.  For example, `export BASE_MODEL_PATH="models/vicuna-7b/"` .
TUNED_MODEL_PATH: Set the path to the tuned model.  For example, `export TUNED_MODEL_PATH="models/vicuna-7b-CrimeKGAssitantClean/"` .
BASE_MODEL_NAME: Set the name of the base model.  For example, `export PROJECT_NAME="vicuna-7b"` .
TUNED_MODEL_NAME: Set the name of the tuned model.  For example, `export PROJECT_NAME="vicuna-7b-CrimeKGAssitantClean"` .
PROJECT_NAME: Set the name of the project.  For example, `export PROJECT_NAME="vicuna-7b-law"` .
CATEGORY: Set the category. For example, `export CATEGORY="Legal Q&A"` .
````

Finally, run the one-click evaluation script:

```bash
source eval/script/run_eval.sh
```

## Workflow
1. **Extract Subset from Source Test Data**: The script uses a random or stratified sampling method to draw a representative subset from the source test dataset.
2. **Generate Test Questions with GPT**: The chosen subset serves as a seed for the GPT model to generate additional test questions, thereby expanding the test dataset.
3. **Answer Generation by Base and Fine-tuned Models**: Both the base model and the fine-tuned model use the extended test dataset to generate corresponding answers.
4. **Performance Evaluation by GPT**: The answers generated by both models are evaluated by the GPT model. This evaluation produces a score that represents the effectiveness of each model.
5. **Visualize Review Result**: The evaluation and scoring data is processed and transformed into a format suitable for displaying on a frontend website.

## Extract Subset from Source Test Data

A carefully curated subset of the source test data is drawn using random or stratified sampling techniques to represent the diversity of the data. This subset serves as the seed data for the GPT model to generate test questions.

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ input
                └─ vicuna-7b-law
                    └─ source_data.json
```

### Sourse Test Data Example

```json
[
    {
        "output": "年9月到明年8月底，租期一年。",
        "input": "你好。我昨晚刚租了房子，签了房屋租赁合同，付一押一，合同签的是今年9.?"
    },
    {
        "output": "是的，无期之前的年数算作有期刑的一部分。",
        "input": "你好，判无期之前住了4年，转有期之后，无期之前的年数在有期之内吗?"
    },
    {
        "output": "发生交通事故应该去事故发生地的基层人民法院起诉。",
        "input": "发生交通事故应该去哪个法院起诉?"
    },
    {
        "output": "请您提供更具体的情况，无法回答。同时，强烈建议您立即向当地公安机关报案，以维护自己的合法权益。",
        "input": "我遇到一个流氓无赖，起先加了好友，聊天以后有好感，就见面了，?"
    }
]
```

## Generate Test Questions with GPT

The GPT model uses the seed data to generate a broad range of test questions, effectively expanding the diversity of the test dataset. The questions generated during this stage assess the models' comprehension, contextual understanding, and response generation capabilities.

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ question
                └─ vicuna-7b-law
                    └─ example_questions.json
                    └─ questions.jsonl
```
### Randomly Picked Seed Questions
```json
[
    {
        "output": "是的，无期之前的年数算作有期刑的一部分。",
        "input": "你好，判无期之前住了4年，转有期之后，无期之前的年数在有期之内吗?"
    },
    {
        "output": "请您提供更具体的情况，无法回答。同时，强烈建议您立即向当地公安机关报案，以维护自己的合法权益。",
        "input": "我遇到一个流氓无赖，起先加了好友，聊天以后有好感，就见面了，?"
    }
]
```

### GPT-Generated Qusestions with Seed Question

```json
{"question_id": 1, "text": "产证是我和母亲的名字，4年前母亲过世，今年初外婆也过世，都没留遗嘱，我舅舅阿姨能分我的房产吗？?", "category": "法律问答"}
{"question_id": 2, "text": "诉中财产保全保金额4O万，需要多少押金?", "category": "法律问答"}
{"question_id": 42, "text": "如果一名工人在工作场所受伤，是否可以获得赔偿？", "category": "法律问答_GPT"}
{"question_id": 43, "text": "如果一个人无意中捡到了一万元现金，这个人需要承担什么样的法律责任？", "category": "法律问答_GPT"}
```


## Answer Generation by Base and Fine-tuned Models

The base and fine-tuned models process the test questions to generate responses. These responses are then used to compare and evaluate the performance of the models, providing a direct measure of the effectiveness of the fine-tuning process.

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ answer
                └─ vicuna-7b-law
                    └─ answer_vicuna-7b-CrimeKGAssitantClean.jsonl
                    └─ answer_vicuna-7b.jsonl
```

## Performance Evaluation by GPT
The answers generated by both the base and the fine-tuned models are fed into a GPT model for evaluation. This process produces a quantitative score that reflects each model's performance and effectiveness.

```bash
Easy-LLM-Evaluator
    └─ eval
        └─ table
            └─ review
                └─ vicuna-7b-law
                    └─ review_vicuna-7b-CrimeKGAssitantClean_vicuna-7b.jsonl
```
## Visualize Review Result

The evaluation and scoring data is transformed into a format suitable for visualization on a frontend web interface. This step helps to translate complex performance metrics into easy-to-understand visual data, providing a comprehensive overview of the models' performance.

<p align="center">
    <img src="./assets/eval.jpg" alt="logo">
</p>

<p align="center">
    <img src="./assets/eval_gpt.jpg" alt="logo">
</p>

